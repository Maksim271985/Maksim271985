{"cells":[{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from datetime import datetime\n","import pandas as pd\n","import time"],"metadata":{"id":"-0K85hhevvG-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url = 'https://habr.com/ru/search/'\n","# Список запросов, по которым будем искать статьи\n","list_post = ['python', 'анализ данных']\n","\n","# Определяем функцию get_post, которая принимает список запросов query\n","def get_post (query):\n","# Создаем пустой датафрейм для хранения результатов\n","  df_post =pd.DataFrame()\n","# Создаем пустой список для хранения ссылок на статьи\n","  all_link = []\n","# Начинаем цикл по каждому запросу из списка query\n","  for post in query:\n","    params = {  'page': 1, 'q': post } # Формируем параметры запроса: page — номер страницы (всегда 1), q — текущий запрос.\n","    res = requests.get(url, params=params) # Отправляем GET-запрос на сайт Habr с указанными параметрами\n","    time.sleep(0.3) # Делаем небольшую паузу (0.3 секунды) между запросами, чтобы не перегружать сервер\n","    soup = BeautifulSoup(res.text) # Создаем объект BeautifulSoup для парсинга HTML-кода страницы\n","    articles = soup.find_all('div', class_='tm-article-snippet') # Находим все блоки статей на странице\n","\n","# Начинаем цикл по каждому блоку статьи\n","    for el in articles:\n","\n","      link = el.find('a', class_='tm-title__link').get('href') # Находим ссылку на статью и извлекаем её\n","      all_link.append('https://habr.com'+link) # Добавляем полную ссылку на статью в список all_link\n","\n","  all_link= set(all_link) # Преобразуем список ссылок в множество, чтобы удалить дубликаты\n","\n","# формируем DataFrame\n","  for link in all_link: # Начинаем цикл по каждой уникальной ссылке\n","    soup = BeautifulSoup(requests.get(link).text) # Отправляем запрос на страницу статьи и создаем объект BeautifulSoup для парсинга\n","\n","    try:\n","      title = soup.find('h1', 'tm-title tm-title_h1').text.strip() # Находим заголовок статьи и очищаем его от лишних пробелов\n","      date =  pd.to_datetime(soup.find('time')['title']).date() # Находим дату публикации и преобразуем её в формат даты\n","      row = {'date': date, 'title': title, 'link': link} # Создаем словарь с данными статьи\n","      df_post = pd.concat([ df_post, pd.DataFrame([row])] ) # Добавляем строку с данными в датафрейм\n","    except:\n","      pass\n","\n","  return df_post.reset_index(drop=True)\n","\n","res = get_post(list_post)\n","res\n","\n","# Сохраняем датафрейм в Excel-файл\n","res.to_excel('habr_posts.xlsx', index=False)\n"],"metadata":{"id":"08PmjHkTugRe"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"16R-dJK8LgohnyagFk5EMEr4tpt9hxZHO","timestamp":1753605629304},{"file_id":"1xlgf2UbcBj7nTR342EKUNj5PQg13YW1o","timestamp":1753516720530}],"authorship_tag":"ABX9TyMMTvQ9P//mqq36wYZeRFMp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}